[
  {
    "question": "What is the foundational model introduced in 2017 that has influenced pre-trained models like BERT and GPT?",
    "answer": "The transformer model.",
    "type": "Factoid",
    "retrieval_ground_truth_chunk_id": "4fe21c2a1aeb0eebd734742d9532555bb0b421003404551a14c084c2f6833453-234ddce8",
    "source_text": "2\nRelated Work\n2.1\nLLMs\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry.\nSubsequently, numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal\nLLMs [19, 20, 21, 22, 23]. In this section, we review these recent advances in LLMs and discuss their connection to\nand distinctions from our work.\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained\nmodels such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence\nof ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and\ntraining data size. This enhancement has resulted in remarkable zero-shot generalization capabilities, allowing these\nmodels to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs\nsuch as OPT [28], BLOOM [18], PaLM [17], and Llama [4]. Particularly, Llama2 [4] is an open-source LLM that\nachieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and\nFalcon, with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization\n[28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two\ntrillion tokens from publicly available sources.\nMultimodal LLMs. "
  },
  {
    "question": "How many tokens does Llama2's pre-training data consist of?",
    "answer": "UNANSWERABLE: The answer is not contained in the provided text.",
    "type": "Out-of-scope",
    "retrieval_ground_truth_chunk_id": "4fe21c2a1aeb0eebd734742d9532555bb0b421003404551a14c084c2f6833453-234ddce8",
    "source_text": "2\nRelated Work\n2.1\nLLMs\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry.\nSubsequently, numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal\nLLMs [19, 20, 21, 22, 23]. In this section, we review these recent advances in LLMs and discuss their connection to\nand distinctions from our work.\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained\nmodels such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence\nof ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and\ntraining data size. This enhancement has resulted in remarkable zero-shot generalization capabilities, allowing these\nmodels to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs\nsuch as OPT [28], BLOOM [18], PaLM [17], and Llama [4]. Particularly, Llama2 [4] is an open-source LLM that\nachieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and\nFalcon, with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization\n[28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two\ntrillion tokens from publicly available sources.\nMultimodal LLMs. "
  },
  {
    "question": "How many epochs of instruct-tuning are conducted after pre-training?",
    "answer": "Three epochs of instruct-tuning.",
    "type": "Factoid",
    "retrieval_ground_truth_chunk_id": "cbf4d86ca8c4fecb47cd24c50f5a27e27fb7f0cfd0799de5877e18526f155e26-c0df6132",
    "source_text": "We conduct one epoch of pre-training, followed by three epochs of instruct-tuning,\nconsidering available computing resources.\nThe maximum sequence length, or context length, is consistently set to 1,024 for both versions during the entire training\nprocess. The DocLLM-7B models are trained with 16-bit mixed precision on 8 24GB A10g GPUs using fully sharded\ndata parallelism, implemented with the accelerate library.4 The DocLLM-1B model, on the other hand, is trained on a\nsingle 24GB A10g GPU.\n4.3\nDownstream Evaluation\nExperimental settings. We investigate two experimental settings:\n\u2022 Same Datasets, Different Splits (SDDS): Following previous work in VRDU [34, 59, 33, 12, 31, 32], we\nfirst evaluate DocLLM on the unseen test split (or dev split when test split is unavailable) of each of the 16\ndatasets composing the instruction-tuning data. The motivation behind this very typical setting is to check how\nDocLLM performs when tasks and domains supposedly stay the same from train to test.\n\u2022 Same Tasks, Different Datasets (STDD): Following [40, 41, 60, 61], we also evaluate DocLLM on held-out\ndatasets. More precisely, we instruction-tune the pre-trained checkpoint of DocLLM on prompts from 11 of\nthe 16 datasets considered in SDDS, then evaluate DocLLM on the test split of the remaining three datasets.\nThe rationale behind this evaluation setting is to assess the performance of DocLLM when tasks are unchanged\nbut domains and layouts differ from train to test. We believe examining this setting in the DocAI field is\nrelevant because industry use cases usually encountered in practice revolve around VQA, KIE, and CLS,\nwhile document characteristics tend to change more often in production. We specifically isolate DocVQA,\nKLC, and BizDocs for STDD evaluation in order to (1) exclude at least one dataset per task from SFT when\npossible, (2) leave enough datapoints per task in the training split of the instruction-tuning data, (3) avoid\ndata leakage (certain datasets were obtained from the same sources), and (4) benchmark models on popular\nyet challenging datasets when possible. Due to the high cost of instruction-tuning, we were not able to run\nadditional experiments with different held-out datasets.\n"
  },
  {
    "question": "What are the two main categories of multimodal LLMs?",
    "answer": "General-purpose multimodal LLMs and models tailored for visually-rich document understanding.",
    "type": "Factoid",
    "retrieval_ground_truth_chunk_id": "4fe21c2a1aeb0eebd734742d9532555bb0b421003404551a14c084c2f6833453-129d62ed",
    "source_text": "Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input.\nThese models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models\nthat are tailored for visually-rich document understanding [31, 32, 33, 34, 12]. The general-purpose multimodal LLMs\nexhibit promising performance in identifying and reasoning with image information. However, they have not yet been\nvigorously evaluated on VRDU tasks. As an example, the GPT-4 Technical Report [16] highlights diverse multimodal\ntest cases, such as explaining meme picture distinctiveness, but very few examples are included for visual document\nuse cases. Prior to the advent of large language models, fine-tune-based models relying on vision only were less\neffective than layout (and vision) modality models in processing visual documents. For example, models like UDOP\n[12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But\nsuch models require task- and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent\nmPLUG-DocOwl [31] and UReader [32], built upon LLMs, undergo instruction finetuning on a diverse set of VRDU,\nvisual, and textual datasets, and exhibit impressive zero-shot generalization capabilities. Hence, we include those as\nbaselines in our evaluation in Section 4.\nDespite the remarkable performance of LLMs, unimodal models aren\u2019t equipped to process multimodal input, and\nmultimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM,\naddresses these challenges by explicitly modeling spatial layouts and text semantics, enabling effective comprehension\nof visual documents. Notably, DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\ntext semantics, avoiding the expensive vision encoder, resulting in a more compact model and efficient processing time.\n2.2\nLLM Architectures\nAutoregressive Infilling. There are two main autoregressive infilling approaches: \u201cfill-in-the-middle\u201d (FIM) where a\nsingle span is sampled, and \u201cblank infilling\u201d with multiple spans.\nThe OpenAI FIM approach [36] uses the template (prefix, middle, suffix) to divide a document into three\nsegments. Next, these segments are reorganized into (prefix, suffix, middle), enabling the model to predict the\nmiddle segment. This process relies on three special tokens, [PRE], [SUF], and [MID], which structure a document as:\n[PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction, while the other two special\ntokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text\nwhere the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible\ngeneration at arbitrary positions, unconstrained by a predefined generation order. In contrast, approaches like GLM\n[15] sample multiple spans for infilling. For each blank to be infilled, a pair of special tokens is used: [blank_mask]\nand [start_to_fill]. The multiple spans not only require special tokens but also global indicators to distinguish\nwhich middle span the model should infill. This global indicator is implemented with 1D token positions, ensuring that\neach pair of the two special tokens, i.e., [blank_mask] and [start_to_fill], share the same positions. We adopt a\nsimilar infilling object with the goal to prevent disconnected next-token predictions while avoiding breaking sparse\ndocuments into very short segments, e.g., word pieces and/or phrase pieces.\n"
  },
  {
    "question": "What is the performance of GPT-4+OCR on the DocVQA dataset in the STDD setting?",
    "answer": "UNANSWERABLE: The answer is not contained in the provided text.",
    "type": "Out-of-scope",
    "retrieval_ground_truth_chunk_id": "ab8be0adccbbb1aad5b5b7250ad1fc4aee765c5948e6eb8e91494898d226019f-240e6e94",
    "source_text": "Table 6: Performance comparison on three held-out VRDU datasets in the STDD setting against non-multimodal LLMs.\nModel\nSize\nSetting\nDocVQA\nKLC\nBizDocs\nVQA\nKIE\nVQA\nKIE\nCLS\nGPT-4+OCR\n\u223c1T\nZS\n82.8\n45.9\n76.4\n66.1\n84.9\nLlama2+OCR\n7B\nZS\n47.4\n27.8\n48.4\n10.8\n40.9\nDocLLM-1B\n1B\nSTDD\n53.5\n40.1\n65.5\n63.0\n20.8\nDocLLM-7B\n7B\nSTDD\n63.4\n49.9\n73.3\n72.6\n31.1\n(a) Causal decoder\n(b) Prefix decoder\nFigure 3: A simplified illustration of attention masks for causal-decoder and prefix-decoder for block infilling.\nDisentangled Spatial Attention. To measure the effect of disentangled spatial attention on cross-modal interactions,\nwe train the models by setting the \u03bb hyperparameter in Eq 6 to 0 or 1 . Table 7 enumerates the attention combinations,\nand the results suggest that keeping only the spatial-to-spatial interaction (i.e. \u03bbs,s = 1) yields the highest NTP\naccuracy. The performance differences among other configurations, such as text-to-spatial and spatial-to-text, are subtle.\nNotably, the vanilla text-only self-attention mechanism yields the lowest NTP accuracy, underlining the importance\nof incorporating spatial features for understanding documents with rich layouts. For all experiments in Section 4, we\ntherefore set \u03bbs,s = 1, \u03bbs,t = 0, and \u03bbt,s = 0. "
  }
]