job_name: "alpaca-llama2-finetune"
base_model_name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
dataset_path: "data/prepared_datasets/alpaca_train.jsonl"
eval_dataset_path: "data/prepared_datasets/alpaca_eval.jsonl"
output_base_dir: "models/fine_tuned"
hyperparameters:
  lora_rank: 8
  learning_rate: 0.0002
  epochs: 3
  batch_size: 4
  lora_alpha: 16
  lora_dropout: 0.05
  gradient_accumulation_steps: 1
  logging_steps: 10
  save_steps: 500
  save_total_limit: 1
