# app/api/training.py
import logging
from pathlib import Path

from fastapi import APIRouter, Depends, HTTPException, status, Body, Query
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, insert
from typing import List, Dict, Any, Optional
from uuid import uuid4
from datetime import datetime

from app.schemas.fine_tuning import FineTuningJobConfig, FineTuningStatus, FineTunedModel
from app.db import get_db
from app.db.models import fine_tuned_models_table

from app.training.trainer_tasks import fine_tune_llm_task

from app.core.security import PermissionChecker
from app.schemas.security import User

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/api/v1/training", tags=["Fine-tuning API"])

@router.post("/apply", response_model=FineTunedModel, status_code=status.HTTP_202_ACCEPTED)
async def apply_fine_tuning_job_config(
    job_config: FineTuningJobConfig, # FastAPI will automatically parse JSON body into this Pydantic model
    current_user: User = Depends(PermissionChecker(["fine_tune:initiate"])), # Requires 'fine_tune:initiate' permission
    db: AsyncSession = Depends(get_db), # Async database session
):
    """
    Submits a new fine-tuning job configuration for asynchronous processing.
    The actual training starts in a background Celery task.
    Requires: 'fine_tune:initiate' permission.
    """
    # A unique adapter_id will be generated by the LLMFineTuner in the Celery task,
    # but we need to pass the job_config directly.
    # The LLMFineTuner handles the initial DB record creation.

    logger.info(f"API: User '{current_user.username}' submitted fine-tuning job '{job_config.job_name}'.")

    try:
        # We don't create the DB record here directly. The LLMFineTuner (in the Celery task)
        # will create the initial PENDING record and update it throughout the process.
        # We just need to trigger the Celery task with the job_config and user_id.

        # Celery tasks usually take dictionaries for arguments
        fine_tune_llm_task.delay(job_config.model_dump(), current_user.id)

        # For immediate API response, we return a placeholder representation of the job.
        # In a real system, you might retrieve the immediately created PENDING record
        # from the database or construct a simplified response.
        # For simplicity, let's assume the task will create the record,
        # and we can acknowledge reception here.
        # A more robust approach might be:
        # 1. Create a PENDING record in DB *here*.
        # 2. Pass the generated adapter_id to the Celery task.
        # 3. Return the PENDING record data to the user.

        # For now, as per LLMFineTuner design, it creates the record.
        # We can return a simplified response or fetch the newly created (PENDING) record
        # if we make a quick synchronous call or retrieve it from task status.
        # For this example, let's just return a basic acknowledgement for the API.
        # (Note: In a production app, you might want to wait for the first DB insert
        # to get the 'adapter_id' to return to the user immediately.)

        # Let's adjust slightly: LLMFineTuner's fine_tune_llm returns the initial record.
        # We can't call it directly in async endpoint.
        # So, for now, we'll confirm the task is launched.
        # The client will need to query GET /jobs/{job_id} for status.

        # To provide an immediate FineTunedModel response, a common pattern is:
        # 1. Generate adapter_id here.
        generated_adapter_id = str(uuid4())
        # 2. Insert PENDING record into DB here.
        new_job_data = {
            "adapter_id": generated_adapter_id,
            "job_name": job_config.job_name,
            "base_model_name": job_config.base_model_name,
            "adapter_path": str(Path(job_config.output_base_dir) / job_config.job_name / generated_adapter_id),
            "training_dataset_id": job_config.dataset_path,
            "training_status": FineTuningStatus.PENDING.value,
            "training_logs_path": str(Path(job_config.output_base_dir) / job_config.job_name / generated_adapter_id / "training_logs.txt"),
            "hyperparameters": job_config.hyperparameters.model_dump(),
            "created_by_user_id": current_user.id,
            "created_at": datetime.utcnow(),
            "updated_at": datetime.utcnow(),
        }
        stmt = insert(fine_tuned_models_table).values(**new_job_data).returning(fine_tuned_models_table)
        result = await db.execute(stmt)
        created_job_row = result.mappings().first()
        await db.commit()

        # 3. Pass the generated adapter_id *and* job_config to the Celery task.
        # The trainer will update this existing record.
        job_config_dict_with_adapter_id = job_config.model_dump()
        job_config_dict_with_adapter_id['adapter_id'] = generated_adapter_id # Pass the generated ID
        fine_tune_llm_task.delay(job_config_dict_with_adapter_id, current_user.id)

        logger.info(f"API: Fine-tuning job '{job_config.job_name}' with ID '{generated_adapter_id}' dispatched successfully.")
        return FineTunedModel.model_validate(created_job_row)

    except Exception as e:
        await db.rollback() # Rollback if database insertion fails
        logger.error(f"API: Failed to submit fine-tuning job '{job_config.job_name}': {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to submit fine-tuning job: {e}")

@router.get("/jobs/{adapter_id}", response_model=FineTunedModel)
async def get_fine_tune_job_status(
    adapter_id: str, # The unique adapter_id for the job
    current_user: User = Depends(PermissionChecker(["fine_tune:read_status"])), # Requires 'fine_tune:read_status' permission
    db: AsyncSession = Depends(get_db),
):
    """
    Retrieves the current status and detailed metadata for a specific fine-tuning job.
    Requires: 'fine_tune:read_status' permission.
    """
    stmt = select(fine_tuned_models_table).where(fine_tuned_models_table.c.adapter_id == adapter_id)
    result = await db.execute(stmt)
    job_data = result.mappings().first() # Fetch a single row
    if not job_data:
        raise HTTPException(status_code=404, detail=f"Fine-tuning job with ID '{adapter_id}' not found.")
    return FineTunedModel.model_validate(job_data) # Convert SQLAlchemy row to Pydantic model

@router.get("/models", response_model=List[FineTunedModel])
async def list_fine_tuned_models(
    status_filter: Optional[FineTuningStatus] = Query(None, description="Filter models by training status (e.g., 'completed')."),
    base_model_name: Optional[str] = Query(None, description="Filter models by the base LLM model name (e.g., 'ollama/llama2')."),
    job_name: Optional[str] = Query(None, description="Filter models by the user-defined job name."),
    limit: int = Query(100, ge=1, le=1000, description="Maximum number of models to return."),
    offset: int = Query(0, ge=0, description="Number of models to skip (for pagination)."),
    current_user: User = Depends(PermissionChecker(["fine_tune:list_models"])), # Requires 'fine_tune:list_models' permission
    db: AsyncSession = Depends(get_db),
):
    """
    Lists available fine-tuned models and their metadata, with filtering and pagination options.
    Requires: 'fine_tune:list_models' permission.
    """
    stmt = select(fine_tuned_models_table).order_by(fine_tuned_models_table.c.created_at.desc())

    # Apply filters based on query parameters
    if status_filter:
        stmt = stmt.where(fine_tuned_models_table.c.training_status == status_filter.value)
    if base_model_name:
        stmt = stmt.where(fine_tuned_models_table.c.base_model_name.ilike(f"%{base_model_name}%")) # Case-insensitive search
    if job_name:
        stmt = stmt.where(fine_tuned_models_table.c.job_name.ilike(f"%{job_name}%"))

    # Apply pagination
    stmt = stmt.limit(limit).offset(offset)

    result = await db.execute(stmt)
    models_data = result.mappings().fetchall() # Fetch all matching rows
    return [FineTunedModel.model_validate(row) for row in models_data] # Convert to Pydantic models